{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0106907-c22b-43a7-9e12-a8c1e8fabe93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸš€ Proyecto: AnÃ¡lisis de Ventas Multicanal\n",
    "## Databricks Free Edition (Serverless)\n",
    "\n",
    "### Objetivos del Proyecto:\n",
    "- Entender la arquitectura de Spark (Driver, Executors, Lazy Evaluation)\n",
    "- Dominar lectura/escritura en mÃºltiples formatos (CSV, JSON, Parquet)\n",
    "- Aplicar JOINs complejos, Subqueries y CTEs\n",
    "- Implementar Window Functions avanzadas\n",
    "- Crear visualizaciones nativas en Databricks\n",
    "\n",
    "### âš ï¸ CaracterÃ­sticas de Databricks Free Edition:\n",
    "- **Serverless Compute**: No necesitas crear clusters, el compute se asigna automÃ¡ticamente\n",
    "- **Unity Catalog**: Almacenamiento con Volumes (`/Volumes/catalog/schema/volume/`)\n",
    "- **Lenguajes**: Solo Python y SQL (no R ni Scala)\n",
    "- **Cuotas**: LÃ­mites diarios de uso\n",
    "- **SQL Warehouse**: 2X-Small incluido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82cc5593-8c7b-447d-a794-b8a2fc849f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ”§ ConfiguraciÃ³n Inicial\n",
    "\n",
    "## Crear el Volume para almacenar archivos\n",
    "\n",
    "Antes de empezar, necesitas crear un Volume en Unity Catalog:\n",
    "\n",
    "**OpciÃ³n 1 - Desde la UI:**\n",
    "1. Ve a **Catalog** en el menÃº lateral\n",
    "2. Selecciona tu catÃ¡logo (por defecto: `main`)\n",
    "3. Selecciona tu schema (por defecto: `default`)\n",
    "4. Clic en **Create** â†’ **Volume**\n",
    "5. Nombre: `proyecto_ventas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a690c7b-7043-4773-bd6e-f5b1c5fcdd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar en Databricks para crear el Volume\n",
    "# spark.sql(\"CREATE VOLUME IF NOT EXISTS main.default.proyecto_ventas\")\n",
    "# spark.sql(\"SHOW VOLUMES IN main.default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "791d2979-2bc2-406c-a468-21aae36f095a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir la ruta base del Volume\n",
    "# IMPORTANTE: Ajusta esto si tu catÃ¡logo/schema son diferentes\n",
    "CATALOG = \"main\"\n",
    "SCHEMA = \"default\"\n",
    "VOLUME = \"proyecto_ventas\"\n",
    "\n",
    "BASE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "print(f\"ğŸ“ Ruta base del proyecto: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b58ed8-7b0b-45f8-950f-0822a2630f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ“š Parte 1: Arquitectura de Apache Spark\n",
    "\n",
    "## 1.1 Driver y Executors\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      SPARK APPLICATION                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\n",
    "â”‚  â”‚     DRIVER      â”‚  â† Coordina la ejecuciÃ³n               â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â† Mantiene SparkContext               â”‚\n",
    "â”‚  â”‚  â”‚ SparkCtx  â”‚  â”‚  â† Divide trabajo en Tasks             â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â† Recolecta resultados                â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
    "â”‚           â”‚                                                  â”‚\n",
    "â”‚           â–¼                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚         SERVERLESS RESOURCE MANAGER          â”‚            â”‚\n",
    "â”‚  â”‚     (Databricks gestiona automÃ¡ticamente)    â”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚           â”‚                                                  â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\n",
    "â”‚     â–¼           â–¼             â–¼                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”                           â”‚\n",
    "â”‚  â”‚ Exec â”‚   â”‚ Exec â”‚     â”‚ Exec â”‚  â† Ejecutan Tasks         â”‚\n",
    "â”‚  â”‚  1   â”‚   â”‚  2   â”‚     â”‚  N   â”‚  â† Almacenan datos        â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜  â† Procesan particiones   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**En Free Edition Serverless**: Databricks asigna recursos automÃ¡ticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d0c36be-3b14-4130-99ff-6541e922b065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar la configuraciÃ³n de Spark (Serverless)\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURACIÃ“N DE SPARK - FREE EDITION SERVERLESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# InformaciÃ³n del SparkContext\n",
    "print(f\"\\nğŸ“Œ Spark Version: {spark.version}\")\n",
    "print(f\"ğŸ“Œ App Name: {spark.sparkContext.appName}\")\n",
    "\n",
    "# Configuraciones disponibles\n",
    "configs_a_revisar = [\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.databricks.clusterUsageTags.clusterName\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“Š Configuraciones disponibles:\")\n",
    "for config in configs_a_revisar:\n",
    "    try:\n",
    "        value = spark.conf.get(config)\n",
    "        print(f\"   {config}: {value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   {config}: (gestionado por serverless)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ En Serverless, Databricks optimiza los recursos automÃ¡ticamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11d3aeb-ff50-4218-8c90-62d087c1f15e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 Lazy Evaluation - DemostraciÃ³n PrÃ¡ctica\n",
    "\n",
    "Spark NO ejecuta transformaciones inmediatamente. Solo las ejecuta cuando hay una **acciÃ³n**.\n",
    "\n",
    "| Transformaciones (Lazy) | Acciones (Eager) |\n",
    "|------------------------|------------------|\n",
    "| select, filter, join   | count, collect   |\n",
    "| groupBy, orderBy       | show, write      |\n",
    "| withColumn, drop       | first, take      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "601161fe-ae08-4dcc-8b9a-42ffa8c1180a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "import time\n",
    "\n",
    "# Crear un DataFrame para demostrar lazy evaluation\n",
    "print(\"ğŸ”„ Creando DataFrame...\")\n",
    "df_demo = spark.range(1000000)  # 1 millÃ³n de filas\n",
    "\n",
    "# TRANSFORMACIONES - No se ejecutan aÃºn (LAZY)\n",
    "print(\"\\nğŸ“ Aplicando transformaciones (LAZY - no se ejecutan aÃºn):\")\n",
    "start = time.time()\n",
    "\n",
    "df_transformed = (df_demo\n",
    "    .withColumn(\"doble\", col(\"id\") * 2)\n",
    "    .withColumn(\"categoria\", when(col(\"id\") < 500000, \"A\").otherwise(\"B\"))\n",
    "    .filter(col(\"id\") % 2 == 0)  # Solo pares\n",
    "    .select(\"id\", \"doble\", \"categoria\")\n",
    ")\n",
    "\n",
    "print(f\"   â±ï¸ Tiempo de definiciÃ³n: {time.time() - start:.4f} segundos\")\n",
    "print(\"   âœ… Las transformaciones estÃ¡n DEFINIDAS pero NO EJECUTADAS\")\n",
    "\n",
    "# ACCIÃ“N - AquÃ­ SÃ se ejecuta todo el pipeline\n",
    "print(\"\\nğŸš€ Ejecutando ACCIÃ“N (count):\")\n",
    "start = time.time()\n",
    "total = df_transformed.count()\n",
    "print(f\"   â±ï¸ Tiempo de ejecuciÃ³n: {time.time() - start:.4f} segundos\")\n",
    "print(f\"   ğŸ“Š Total de registros: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5a44c2-f535-4cb8-994c-00d5600fab7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver el plan de ejecuciÃ³n (Explain Plan)\n",
    "print(\"ğŸ“‹ PLAN DE EJECUCIÃ“N (Explain):\")\n",
    "print(\"=\" * 60)\n",
    "df_transformed.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5380e03-bc86-446d-ae7b-27bd3b0b8dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ“ Parte 2: GeneraciÃ³n y Almacenamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbda6fe2-4a18-401c-a2ae-f4087ceb872c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    to_date, concat, lit, col, \n",
    "    count, sum as spark_sum, avg, round as spark_round,\n",
    "    collect_set, datediff, when\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# DATOS DE CLIENTES\n",
    "# ============================================================\n",
    "print(\"ğŸ‘¥ Generando datos de CLIENTES...\")\n",
    "\n",
    "nombres = [\"MarÃ­a\", \"Juan\", \"Ana\", \"Carlos\", \"Laura\", \"Pedro\", \"Sofia\", \"Diego\", \"Valentina\", \"AndrÃ©s\",\n",
    "           \"Camila\", \"Luis\", \"Isabella\", \"Miguel\", \"Daniela\", \"JosÃ©\", \"Luciana\", \"Fernando\", \"Paula\", \"Ricardo\"]\n",
    "apellidos = [\"GarcÃ­a\", \"RodrÃ­guez\", \"MartÃ­nez\", \"LÃ³pez\", \"GonzÃ¡lez\", \"HernÃ¡ndez\", \"PÃ©rez\", \"SÃ¡nchez\", \n",
    "             \"RamÃ­rez\", \"Torres\", \"Flores\", \"Rivera\", \"GÃ³mez\", \"DÃ­az\", \"Reyes\", \"Cruz\", \"Morales\", \"Ortiz\"]\n",
    "ciudades = [\"Lima\", \"Arequipa\", \"Trujillo\", \"Cusco\", \"Piura\", \"Chiclayo\", \"Iquitos\", \"Huancayo\"]\n",
    "segmentos = [\"Premium\", \"Regular\", \"BÃ¡sico\"]\n",
    "\n",
    "clientes_data = []\n",
    "for i in range(1, 501):\n",
    "    clientes_data.append((\n",
    "        i,\n",
    "        random.choice(nombres),\n",
    "        random.choice(apellidos),\n",
    "        f\"cliente{i}@email.com\",\n",
    "        random.choice(ciudades),\n",
    "        random.choice(segmentos),\n",
    "        f\"2020-{random.randint(1,12):02d}-{random.randint(1,28):02d}\"\n",
    "    ))\n",
    "\n",
    "schema_clientes = StructType([\n",
    "    StructField(\"cliente_id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"apellido\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"ciudad\", StringType(), True),\n",
    "    StructField(\"segmento\", StringType(), True),\n",
    "    StructField(\"fecha_registro\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_clientes = spark.createDataFrame(clientes_data, schema_clientes)\n",
    "df_clientes = df_clientes.withColumn(\"fecha_registro\", to_date(\"fecha_registro\"))\n",
    "\n",
    "print(f\"   âœ… {df_clientes.count()} clientes generados\")\n",
    "display(df_clientes.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d640df11-8bd4-4dc6-b1b8-dd756c700625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATOS DE PRODUCTOS\n",
    "# ============================================================\n",
    "print(\"ğŸ“¦ Generando datos de PRODUCTOS...\")\n",
    "\n",
    "productos_data = [\n",
    "    (1, \"Laptop HP Pavilion\", \"ElectrÃ³nica\", \"Computadoras\", 2499.99, 50),\n",
    "    (2, \"iPhone 15 Pro\", \"ElectrÃ³nica\", \"Smartphones\", 4999.99, 30),\n",
    "    (3, \"Samsung Galaxy S24\", \"ElectrÃ³nica\", \"Smartphones\", 3799.99, 45),\n",
    "    (4, \"AudÃ­fonos Sony WH-1000XM5\", \"ElectrÃ³nica\", \"Audio\", 1299.99, 100),\n",
    "    (5, \"Monitor LG 27'' 4K\", \"ElectrÃ³nica\", \"Monitores\", 1599.99, 40),\n",
    "    (6, \"Teclado Logitech MX Keys\", \"ElectrÃ³nica\", \"Accesorios\", 449.99, 80),\n",
    "    (7, \"Mouse Logitech MX Master\", \"ElectrÃ³nica\", \"Accesorios\", 399.99, 90),\n",
    "    (8, \"Tablet iPad Air\", \"ElectrÃ³nica\", \"Tablets\", 2799.99, 35),\n",
    "    (9, \"Smartwatch Apple Watch\", \"ElectrÃ³nica\", \"Wearables\", 1999.99, 60),\n",
    "    (10, \"CÃ¡mara Canon EOS R6\", \"ElectrÃ³nica\", \"FotografÃ­a\", 6499.99, 15),\n",
    "    (11, \"Silla ErgonÃ³mica\", \"Muebles\", \"Oficina\", 899.99, 70),\n",
    "    (12, \"Escritorio Standing Desk\", \"Muebles\", \"Oficina\", 1299.99, 25),\n",
    "    (13, \"LÃ¡mpara LED Escritorio\", \"Muebles\", \"IluminaciÃ³n\", 149.99, 150),\n",
    "    (14, \"Mochila para Laptop\", \"Accesorios\", \"Bolsos\", 199.99, 200),\n",
    "    (15, \"Hub USB-C 7 en 1\", \"ElectrÃ³nica\", \"Accesorios\", 179.99, 120),\n",
    "    (16, \"Disco SSD 1TB\", \"ElectrÃ³nica\", \"Almacenamiento\", 349.99, 85),\n",
    "    (17, \"Webcam Logitech 4K\", \"ElectrÃ³nica\", \"Video\", 549.99, 55),\n",
    "    (18, \"MicrÃ³fono Blue Yeti\", \"ElectrÃ³nica\", \"Audio\", 499.99, 65),\n",
    "    (19, \"Router WiFi 6\", \"ElectrÃ³nica\", \"Redes\", 299.99, 75),\n",
    "    (20, \"Power Bank 20000mAh\", \"ElectrÃ³nica\", \"Accesorios\", 129.99, 180)\n",
    "]\n",
    "\n",
    "schema_productos = StructType([\n",
    "    StructField(\"producto_id\", IntegerType(), False),\n",
    "    StructField(\"nombre_producto\", StringType(), True),\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"subcategoria\", StringType(), True),\n",
    "    StructField(\"precio_unitario\", DoubleType(), True),\n",
    "    StructField(\"stock_actual\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_productos = spark.createDataFrame(productos_data, schema_productos)\n",
    "print(f\"   âœ… {df_productos.count()} productos generados\")\n",
    "display(df_productos.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156c5b33-00d8-4718-90b1-0177efa61c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATOS DE TRANSACCIONES (VENTAS)\n",
    "# ============================================================\n",
    "print(\"ğŸ’° Generando datos de TRANSACCIONES...\")\n",
    "\n",
    "canales = [\"Online\", \"Tienda FÃ­sica\", \"App MÃ³vil\", \"Marketplace\", \"TelÃ©fono\"]\n",
    "metodos_pago = [\"Tarjeta CrÃ©dito\", \"Tarjeta DÃ©bito\", \"Efectivo\", \"Transferencia\", \"PayPal\"]\n",
    "estados = [\"Completada\", \"Completada\", \"Completada\", \"Completada\", \"Pendiente\", \"Cancelada\"]\n",
    "\n",
    "transacciones_data = []\n",
    "for i in range(1, 5001):\n",
    "    cliente_id = random.randint(1, 500)\n",
    "    producto_id = random.randint(1, 20)\n",
    "    cantidad = random.randint(1, 5)\n",
    "    precio = [p[4] for p in productos_data if p[0] == producto_id][0]\n",
    "    descuento = random.choice([0, 0, 0, 5, 10, 15, 20])\n",
    "    \n",
    "    transacciones_data.append((\n",
    "        i,\n",
    "        cliente_id,\n",
    "        producto_id,\n",
    "        f\"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
    "        cantidad,\n",
    "        precio,\n",
    "        descuento,\n",
    "        random.choice(canales),\n",
    "        random.choice(metodos_pago),\n",
    "        random.choice(estados)\n",
    "    ))\n",
    "\n",
    "schema_transacciones = StructType([\n",
    "    StructField(\"transaccion_id\", IntegerType(), False),\n",
    "    StructField(\"cliente_id\", IntegerType(), True),\n",
    "    StructField(\"producto_id\", IntegerType(), True),\n",
    "    StructField(\"fecha_transaccion\", StringType(), True),\n",
    "    StructField(\"cantidad\", IntegerType(), True),\n",
    "    StructField(\"precio_unitario\", DoubleType(), True),\n",
    "    StructField(\"descuento_porcentaje\", IntegerType(), True),\n",
    "    StructField(\"canal_venta\", StringType(), True),\n",
    "    StructField(\"metodo_pago\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_transacciones = spark.createDataFrame(transacciones_data, schema_transacciones)\n",
    "df_transacciones = (df_transacciones\n",
    "    .withColumn(\"fecha_transaccion\", to_date(\"fecha_transaccion\"))\n",
    "    .withColumn(\"total_bruto\", col(\"cantidad\") * col(\"precio_unitario\"))\n",
    "    .withColumn(\"monto_descuento\", col(\"total_bruto\") * col(\"descuento_porcentaje\") / 100)\n",
    "    .withColumn(\"total_neto\", col(\"total_bruto\") - col(\"monto_descuento\"))\n",
    ")\n",
    "\n",
    "print(f\"   âœ… {df_transacciones.count()} transacciones generadas\")\n",
    "display(df_transacciones.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee002101-5bab-49a6-b0a1-be41cdd9eb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 Escritura de Datos en MÃºltiples Formatos\n",
    "\n",
    "En Free Edition, usamos **Unity Catalog Volumes** para almacenar archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c7665b-ede1-4855-9048-e6b674ad88ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "print(f\"ğŸ“ Guardando datos en: {BASE_PATH}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR EN CSV\n",
    "# ============================================================\n",
    "csv_path = f\"{BASE_PATH}/csv\"\n",
    "\n",
    "df_clientes.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{csv_path}/clientes\")\n",
    "df_productos.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{csv_path}/productos\")\n",
    "df_transacciones.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{csv_path}/transacciones\")\n",
    "\n",
    "print(f\"âœ… CSV guardado en: {csv_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR EN JSON\n",
    "# ============================================================\n",
    "json_path = f\"{BASE_PATH}/json\"\n",
    "\n",
    "df_clientes.write.mode(\"overwrite\").json(f\"{json_path}/clientes\")\n",
    "df_productos.write.mode(\"overwrite\").json(f\"{json_path}/productos\")\n",
    "df_transacciones.write.mode(\"overwrite\").json(f\"{json_path}/transacciones\")\n",
    "\n",
    "print(f\"âœ… JSON guardado en: {json_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR EN PARQUET (Formato mÃ¡s eficiente)\n",
    "# ============================================================\n",
    "parquet_path = f\"{BASE_PATH}/parquet\"\n",
    "\n",
    "df_clientes.write.mode(\"overwrite\").parquet(f\"{parquet_path}/clientes\")\n",
    "df_productos.write.mode(\"overwrite\").parquet(f\"{parquet_path}/productos\")\n",
    "\n",
    "# Transacciones particionadas por mes\n",
    "df_transacciones_part = df_transacciones.withColumn(\"mes\", month(\"fecha_transaccion\"))\n",
    "df_transacciones_part.write.mode(\"overwrite\").partitionBy(\"mes\").parquet(f\"{parquet_path}/transacciones\")\n",
    "\n",
    "print(f\"âœ… Parquet guardado en: {parquet_path}\")\n",
    "print(f\"   ğŸ“Œ Transacciones particionadas por MES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdcd0ee6-54bd-448c-8071-130ce917ae22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Lectura de Datos desde Diferentes Formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d8e699-b283-4044-9e6c-274cba0849c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LECTURA DESDE CSV\n",
    "# ============================================================\n",
    "print(\"ğŸ“– Leyendo desde CSV...\")\n",
    "\n",
    "clientes_csv = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{csv_path}/clientes\")\n",
    ")\n",
    "\n",
    "print(f\"   Registros leÃ­dos: {clientes_csv.count()}\")\n",
    "print(f\"   Schema inferido:\")\n",
    "clientes_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66563ed1-3432-4a0a-a1fa-2de2703d0baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LECTURA DESDE JSON\n",
    "# ============================================================\n",
    "print(\"ğŸ“– Leyendo desde JSON...\")\n",
    "\n",
    "productos_json = spark.read.json(f\"{json_path}/productos\")\n",
    "\n",
    "print(f\"   Registros leÃ­dos: {productos_json.count()}\")\n",
    "display(productos_json.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02daebb3-7231-443f-a041-d09c744d6dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LECTURA DESDE PARQUET (MÃ¡s eficiente)\n",
    "# ============================================================\n",
    "print(\"ğŸ“– Leyendo desde PARQUET...\")\n",
    "\n",
    "transacciones_parquet = spark.read.parquet(f\"{parquet_path}/transacciones\")\n",
    "\n",
    "print(f\"   Registros leÃ­dos: {transacciones_parquet.count()}\")\n",
    "\n",
    "# Ver las particiones\n",
    "print(f\"\\nğŸ“Š Estructura de particiones por mes:\")\n",
    "display(transacciones_parquet.select(\"mes\").distinct().orderBy(\"mes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b8c2fd-95cf-42cb-a1ec-638b315d61dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ComparaciÃ³n de rendimiento entre formatos\n",
    "import time\n",
    "\n",
    "def medir_lectura(formato, path):\n",
    "    start = time.time()\n",
    "    if formato == \"csv\":\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(path)\n",
    "    elif formato == \"json\":\n",
    "        df = spark.read.json(path)\n",
    "    else:\n",
    "        df = spark.read.parquet(path)\n",
    "    \n",
    "    count = df.count()\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed, count\n",
    "\n",
    "print(\"â±ï¸ COMPARACIÃ“N DE TIEMPOS DE LECTURA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "formatos = [\n",
    "    (\"CSV\", \"csv\", f\"{csv_path}/transacciones\"),\n",
    "    (\"JSON\", \"json\", f\"{json_path}/transacciones\"),\n",
    "    (\"Parquet\", \"parquet\", f\"{parquet_path}/transacciones\")\n",
    "]\n",
    "\n",
    "for nombre, formato, path in formatos:\n",
    "    tiempo, registros = medir_lectura(formato, path)\n",
    "    print(f\"   {nombre:10} â†’ {tiempo:.3f} seg ({registros:,} registros)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Parquet es generalmente mÃ¡s rÃ¡pido por su formato columnar y compresiÃ³n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb7ac69-70dd-42c9-95ec-470ac9f56d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ”— Parte 3: JOINs Complejos, Subqueries y CTEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "102e14c3-f342-409c-85c9-1670785bd665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar datos desde Parquet para el anÃ¡lisis\n",
    "df_clientes = spark.read.parquet(f\"{parquet_path}/clientes\")\n",
    "df_productos = spark.read.parquet(f\"{parquet_path}/productos\")\n",
    "df_transacciones = spark.read.parquet(f\"{parquet_path}/transacciones\")\n",
    "\n",
    "# Crear vistas temporales para SQL\n",
    "df_clientes.createOrReplaceTempView(\"clientes\")\n",
    "df_productos.createOrReplaceTempView(\"productos\")\n",
    "df_transacciones.createOrReplaceTempView(\"transacciones\")\n",
    "\n",
    "print(\"âœ… Vistas SQL creadas: clientes, productos, transacciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db3cd89-c45d-491d-b397-b05a86a649a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 JOINs BÃ¡sicos y Complejos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0611875-0c7f-4374-a758-72eda0d95af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INNER JOIN - Transacciones con informaciÃ³n completa\n",
    "# ============================================================\n",
    "print(\"ğŸ”— INNER JOIN - Vista completa de ventas\")\n",
    "\n",
    "df_ventas_completas = (df_transacciones\n",
    "    .join(df_clientes, \"cliente_id\", \"inner\")\n",
    "    .join(df_productos, \"producto_id\", \"inner\")\n",
    "    .select(\n",
    "        \"transaccion_id\",\n",
    "        \"fecha_transaccion\",\n",
    "        concat(col(\"nombre\"), lit(\" \"), col(\"apellido\")).alias(\"cliente\"),\n",
    "        \"ciudad\",\n",
    "        \"segmento\",\n",
    "        \"nombre_producto\",\n",
    "        \"categoria\",\n",
    "        \"canal_venta\",\n",
    "        \"cantidad\",\n",
    "        \"total_neto\",\n",
    "        \"estado\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_ventas_completas.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5863921-6da9-464b-aaa2-41a5a7a2350f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEFT JOIN - Clientes con o sin compras\n",
    "# ============================================================\n",
    "print(\"ğŸ”— LEFT JOIN - AnÃ¡lisis de clientes (con y sin compras)\")\n",
    "\n",
    "# Resumen de compras por cliente\n",
    "df_resumen_cliente = (df_transacciones\n",
    "    .filter(col(\"estado\") == \"Completada\")\n",
    "    .groupBy(\"cliente_id\")\n",
    "    .agg(\n",
    "        count(\"transaccion_id\").alias(\"total_compras\"),\n",
    "        spark_round(spark_sum(\"total_neto\"), 2).alias(\"total_gastado\"),\n",
    "        spark_round(avg(\"total_neto\"), 2).alias(\"ticket_promedio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# LEFT JOIN para incluir clientes sin compras\n",
    "df_clientes_analisis = (df_clientes\n",
    "    .join(df_resumen_cliente, \"cliente_id\", \"left\")\n",
    "    .fillna(0, [\"total_compras\", \"total_gastado\", \"ticket_promedio\"])\n",
    "    .orderBy(col(\"total_gastado\").desc())\n",
    ")\n",
    "\n",
    "print(f\"Total clientes: {df_clientes_analisis.count()}\")\n",
    "print(f\"Clientes sin compras: {df_clientes_analisis.filter(col('total_compras') == 0).count()}\")\n",
    "display(df_clientes_analisis.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8ff2ac-8fb1-4c52-be41-1cb56cc771a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AnÃ¡lisis de clientes multicanal\n",
    "# ============================================================\n",
    "print(\"ğŸ”— AnÃ¡lisis de clientes multicanal\")\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df_multicanal = (df_transacciones\n",
    "    .filter(col(\"estado\") == \"Completada\")\n",
    "    .groupBy(\"cliente_id\")\n",
    "    .agg(\n",
    "        countDistinct(\"canal_venta\").alias(\"canales_usados\"),\n",
    "        collect_set(\"canal_venta\").alias(\"lista_canales\")\n",
    "    )\n",
    "    .filter(col(\"canales_usados\") > 1)\n",
    "    .join(df_clientes.select(\"cliente_id\", \"nombre\", \"apellido\", \"segmento\"), \"cliente_id\")\n",
    "    .orderBy(col(\"canales_usados\").desc())\n",
    ")\n",
    "\n",
    "print(f\"Clientes que compran en mÃºltiples canales: {df_multicanal.count()}\")\n",
    "display(df_multicanal.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7719a9d-86f1-477c-abee-dba1831c7447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 Subqueries y CTEs en SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bdacd46-ce7f-4465-aac5-534080acfaef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUBQUERY en WHERE - Clientes con compras mayores al promedio\n",
    "# ============================================================\n",
    "\n",
    "df_subquery = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.cliente_id,\n",
    "        c.nombre,\n",
    "        c.apellido,\n",
    "        c.ciudad,\n",
    "        c.segmento,\n",
    "        ROUND(SUM(t.total_neto), 2) as total_gastado\n",
    "    FROM clientes c\n",
    "    INNER JOIN transacciones t ON c.cliente_id = t.cliente_id\n",
    "    WHERE t.estado = 'Completada'\n",
    "    GROUP BY c.cliente_id, c.nombre, c.apellido, c.ciudad, c.segmento\n",
    "    HAVING SUM(t.total_neto) > (\n",
    "        SELECT AVG(total_cliente)\n",
    "        FROM (\n",
    "            SELECT cliente_id, SUM(total_neto) as total_cliente\n",
    "            FROM transacciones\n",
    "            WHERE estado = 'Completada'\n",
    "            GROUP BY cliente_id\n",
    "        )\n",
    "    )\n",
    "    ORDER BY total_gastado DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š SUBQUERY - Clientes con compras mayores al promedio:\")\n",
    "display(df_subquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db54de2d-621c-4128-852a-75853b2d5a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CTE (Common Table Expression) - AnÃ¡lisis jerÃ¡rquico de ventas\n",
    "# ============================================================\n",
    "\n",
    "df_cte = spark.sql(\"\"\"\n",
    "    WITH ventas_por_categoria AS (\n",
    "        SELECT \n",
    "            p.categoria,\n",
    "            COUNT(t.transaccion_id) as num_ventas,\n",
    "            ROUND(SUM(t.total_neto), 2) as ingresos_totales,\n",
    "            ROUND(AVG(t.total_neto), 2) as ticket_promedio\n",
    "        FROM transacciones t\n",
    "        INNER JOIN productos p ON t.producto_id = p.producto_id\n",
    "        WHERE t.estado = 'Completada'\n",
    "        GROUP BY p.categoria\n",
    "    ),\n",
    "\n",
    "    ventas_por_canal AS (\n",
    "        SELECT \n",
    "            p.categoria,\n",
    "            t.canal_venta,\n",
    "            ROUND(SUM(t.total_neto), 2) as ingresos_canal\n",
    "        FROM transacciones t\n",
    "        INNER JOIN productos p ON t.producto_id = p.producto_id\n",
    "        WHERE t.estado = 'Completada'\n",
    "        GROUP BY p.categoria, t.canal_venta\n",
    "    ),\n",
    "\n",
    "    ranking_canales AS (\n",
    "        SELECT \n",
    "            categoria,\n",
    "            canal_venta,\n",
    "            ingresos_canal,\n",
    "            ROW_NUMBER() OVER (PARTITION BY categoria ORDER BY ingresos_canal DESC) as ranking\n",
    "        FROM ventas_por_canal\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        vp.categoria,\n",
    "        vp.num_ventas,\n",
    "        vp.ingresos_totales,\n",
    "        vp.ticket_promedio,\n",
    "        rc.canal_venta as canal_principal,\n",
    "        rc.ingresos_canal as ingresos_canal_principal\n",
    "    FROM ventas_por_categoria vp\n",
    "    INNER JOIN ranking_canales rc ON vp.categoria = rc.categoria AND rc.ranking = 1\n",
    "    ORDER BY vp.ingresos_totales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š CTE - AnÃ¡lisis jerÃ¡rquico de ventas:\")\n",
    "display(df_cte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84fb9733-593d-46c1-858a-f5180bf209c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸªŸ Parte 4: Window Functions\n",
    "\n",
    "## 4.1 ROW_NUMBER, RANK, DENSE_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1470d1e9-c8a1-43fb-bbf0-206bb5e94fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    row_number, rank, dense_rank, \n",
    "    lag, lead, \n",
    "    sum as spark_sum, avg as spark_avg,\n",
    "    first, last, ntile, date_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9c77f8b-1cbb-4cc6-9b82-e5771601e6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ROW_NUMBER - Numerar transacciones por cliente\n",
    "# ============================================================\n",
    "print(\"ğŸ”¢ ROW_NUMBER - Orden de compras por cliente\")\n",
    "\n",
    "window_cliente = Window.partitionBy(\"cliente_id\").orderBy(col(\"fecha_transaccion\").desc())\n",
    "\n",
    "df_con_numero = (df_transacciones\n",
    "    .filter(col(\"estado\") == \"Completada\")\n",
    "    .withColumn(\"numero_compra\", row_number().over(window_cliente))\n",
    "    .join(df_clientes.select(\"cliente_id\", \"nombre\", \"apellido\"), \"cliente_id\")\n",
    "    .select(\"cliente_id\", \"nombre\", \"apellido\", \"fecha_transaccion\", \"total_neto\", \"numero_compra\")\n",
    "    .filter(col(\"numero_compra\") <= 3)\n",
    "    .orderBy(\"cliente_id\", \"numero_compra\")\n",
    ")\n",
    "\n",
    "display(df_con_numero.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d04e3a-a76f-42c1-9287-f9c89cdfda1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RANK vs DENSE_RANK - Ranking de productos por ingresos\n",
    "# ============================================================\n",
    "print(\"ğŸ† RANK vs DENSE_RANK - ComparaciÃ³n de rankings\")\n",
    "\n",
    "df_ingresos_producto = (df_transacciones\n",
    "    .filter(col(\"estado\") == \"Completada\")\n",
    "    .join(df_productos.select(\"producto_id\", \"nombre_producto\", \"categoria\"), \"producto_id\")\n",
    "    .groupBy(\"producto_id\", \"nombre_producto\", \"categoria\")\n",
    "    .agg(spark_round(spark_sum(\"total_neto\"), 2).alias(\"ingresos_totales\"))\n",
    ")\n",
    "\n",
    "window_categoria = Window.partitionBy(\"categoria\").orderBy(col(\"ingresos_totales\").desc())\n",
    "\n",
    "df_ranking = (df_ingresos_producto\n",
    "    .withColumn(\"rank\", rank().over(window_categoria))\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_categoria))\n",
    "    .withColumn(\"row_number\", row_number().over(window_categoria))\n",
    "    .orderBy(\"categoria\", \"rank\")\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Œ Diferencias:\")\n",
    "print(\"   - ROW_NUMBER: Siempre Ãºnico (1,2,3,4,5...)\")\n",
    "print(\"   - RANK: Permite empates, salta posiciones (1,2,2,4,5...)\")\n",
    "print(\"   - DENSE_RANK: Permite empates, no salta (1,2,2,3,4...)\")\n",
    "\n",
    "display(df_ranking.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe16ae9-be9f-4a62-9d8e-258447a96c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 LAG y LEAD - AnÃ¡lisis Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c72949-0d02-407b-a05e-f506e89b2af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LAG y LEAD - Comparar ventas entre periodos\n",
    "# ============================================================\n",
    "print(\"ğŸ“ˆ LAG/LEAD - AnÃ¡lisis de tendencias de ventas mensuales\")\n",
    "\n",
    "# Ventas mensuales\n",
    "df_ventas_mes = (df_transacciones\n",
    "    .filter(col(\"estado\") == \"Completada\")\n",
    "    .withColumn(\"anio_mes\", date_format(\"fecha_transaccion\", \"yyyy-MM\"))\n",
    "    .groupBy(\"anio_mes\")\n",
    "    .agg(\n",
    "        count(\"transaccion_id\").alias(\"num_ventas\"),\n",
    "        spark_round(spark_sum(\"total_neto\"), 2).alias(\"ingresos\")\n",
    "    )\n",
    "    .orderBy(\"anio_mes\")\n",
    ")\n",
    "\n",
    "window_temporal = Window.orderBy(\"anio_mes\")\n",
    "\n",
    "df_tendencias = (df_ventas_mes\n",
    "    .withColumn(\"ingresos_mes_anterior\", lag(\"ingresos\", 1).over(window_temporal))\n",
    "    .withColumn(\"ingresos_mes_siguiente\", lead(\"ingresos\", 1).over(window_temporal))\n",
    "    .withColumn(\"variacion_vs_anterior\", \n",
    "        spark_round((col(\"ingresos\") - col(\"ingresos_mes_anterior\")) / col(\"ingresos_mes_anterior\") * 100, 2))\n",
    "    .withColumn(\"tendencia\", \n",
    "        when(col(\"variacion_vs_anterior\") > 5, \"ğŸ“ˆ Crecimiento\")\n",
    "        .when(col(\"variacion_vs_anterior\") < -5, \"ğŸ“‰ Decrecimiento\")\n",
    "        .otherwise(\"â¡ï¸ Estable\"))\n",
    ")\n",
    "\n",
    "display(df_tendencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa49d6c-e024-4c5f-bacb-2ec97086572c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LAG - AnÃ¡lisis de frecuencia de compra por cliente\n",
    "# ============================================================\n",
    "print(\"ğŸ”„ LAG - DÃ­as entre compras por cliente\")\n",
    "\n",
    "window_compras = Window.partitionBy(\"cliente_id\").orderBy(\"fecha_transaccion\")\n",
    "\n",
    "df_frecuencia = (df_transacciones\n",
    "    .filter(col(\"estado\") == \"Completada\")\n",
    "    .withColumn(\"fecha_compra_anterior\", lag(\"fecha_transaccion\", 1).over(window_compras))\n",
    "    .withColumn(\"dias_entre_compras\", \n",
    "        datediff(\"fecha_transaccion\", \"fecha_compra_anterior\"))\n",
    "    .filter(col(\"dias_entre_compras\").isNotNull())\n",
    "    .join(df_clientes.select(\"cliente_id\", \"nombre\", \"segmento\"), \"cliente_id\")\n",
    ")\n",
    "\n",
    "# Promedio de dÃ­as entre compras por segmento\n",
    "df_frecuencia_segmento = (df_frecuencia\n",
    "    .groupBy(\"segmento\")\n",
    "    .agg(\n",
    "        spark_round(spark_avg(\"dias_entre_compras\"), 1).alias(\"promedio_dias_entre_compras\"),\n",
    "        count(\"*\").alias(\"total_recompras\")\n",
    "    )\n",
    "    .orderBy(\"promedio_dias_entre_compras\")\n",
    ")\n",
    "\n",
    "print(\"â±ï¸ Frecuencia de recompra por segmento:\")\n",
    "display(df_frecuencia_segmento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f7974d6-7d5f-434c-b0ee-156b1a1ff5b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 Running Totals y Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa0df94-5191-4664-aa55-92107480317b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUNNING TOTAL - Ventas acumuladas\n",
    "# ============================================================\n",
    "print(\"ğŸ“Š Running Total - Ingresos acumulados por mes\")\n",
    "\n",
    "window_acumulado = Window.orderBy(\"anio_mes\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_acumulado = (df_ventas_mes\n",
    "    .withColumn(\"ingresos_acumulados\", spark_round(spark_sum(\"ingresos\").over(window_acumulado), 2))\n",
    "    .withColumn(\"ventas_acumuladas\", spark_sum(\"num_ventas\").over(window_acumulado))\n",
    ")\n",
    "\n",
    "display(df_acumulado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675d384e-c253-4698-ba95-3ec63ea376b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MOVING AVERAGE - Promedio mÃ³vil de 3 meses\n",
    "# ============================================================\n",
    "print(\"ğŸ“‰ Moving Average - Promedio mÃ³vil de ingresos (3 meses)\")\n",
    "\n",
    "window_moving = Window.orderBy(\"anio_mes\").rowsBetween(-2, 0)\n",
    "\n",
    "df_moving_avg = (df_ventas_mes\n",
    "    .withColumn(\"promedio_movil_3m\", spark_round(spark_avg(\"ingresos\").over(window_moving), 2))\n",
    "    .withColumn(\"diferencia_vs_promedio\", \n",
    "        spark_round(col(\"ingresos\") - col(\"promedio_movil_3m\"), 2))\n",
    ")\n",
    "\n",
    "display(df_moving_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b57e599e-4475-4b7d-9cad-ea97343174fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Window Functions completas en SQL\n",
    "# ============================================================\n",
    "\n",
    "df_window_sql = spark.sql(\"\"\"\n",
    "    WITH ventas_diarias AS (\n",
    "        SELECT \n",
    "            fecha_transaccion,\n",
    "            canal_venta,\n",
    "            COUNT(*) as num_ventas,\n",
    "            ROUND(SUM(total_neto), 2) as ingresos\n",
    "        FROM transacciones\n",
    "        WHERE estado = 'Completada'\n",
    "        GROUP BY fecha_transaccion, canal_venta\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        fecha_transaccion,\n",
    "        canal_venta,\n",
    "        num_ventas,\n",
    "        ingresos,\n",
    "        RANK() OVER (PARTITION BY fecha_transaccion ORDER BY ingresos DESC) as ranking_dia,\n",
    "        LAG(ingresos, 1) OVER (PARTITION BY canal_venta ORDER BY fecha_transaccion) as ingresos_dia_anterior,\n",
    "        SUM(ingresos) OVER (PARTITION BY canal_venta ORDER BY fecha_transaccion \n",
    "                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as acumulado_canal,\n",
    "        ROUND(ingresos * 100.0 / SUM(ingresos) OVER (PARTITION BY fecha_transaccion), 2) as pct_del_dia\n",
    "    FROM ventas_diarias\n",
    "    ORDER BY fecha_transaccion DESC, ranking_dia\n",
    "    LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š Window Functions en SQL:\")\n",
    "display(df_window_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56c88bb-3c53-415a-8988-3c20bbb930d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ“Š Parte 5: Visualizaciones Nativas en Databricks\n",
    "\n",
    "## CÃ³mo crear visualizaciones:\n",
    "1. Ejecuta la celda con `display(df)`\n",
    "2. Clic en el icono **\"+\"** junto a la tabla\n",
    "3. Selecciona **\"Visualization\"**\n",
    "4. Elige el tipo de grÃ¡fico y configura los ejes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df99061-3dac-4d76-98f0-5af1eeb22e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 1: Ventas por Canal (GrÃ¡fico de Barras)\n",
    "# ============================================================\n",
    "# InstrucciÃ³n: Clic en \"+\" â†’ Visualization â†’ Bar Chart\n",
    "# X-axis: canal_venta | Y-axis: ingresos_totales\n",
    "\n",
    "df_ventas_canal = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        canal_venta,\n",
    "        COUNT(*) as numero_ventas,\n",
    "        ROUND(SUM(total_neto), 2) as ingresos_totales,\n",
    "        ROUND(AVG(total_neto), 2) as ticket_promedio\n",
    "    FROM transacciones\n",
    "    WHERE estado = 'Completada'\n",
    "    GROUP BY canal_venta\n",
    "    ORDER BY ingresos_totales DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_ventas_canal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ed4c7b-af72-4a93-b86a-a8bd2b690470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 2: Tendencia Mensual (GrÃ¡fico de LÃ­neas)\n",
    "# ============================================================\n",
    "# InstrucciÃ³n: Line Chart con anio_mes en X-axis\n",
    "\n",
    "df_tendencia_mensual = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_FORMAT(fecha_transaccion, 'yyyy-MM') as anio_mes,\n",
    "        COUNT(*) as numero_ventas,\n",
    "        ROUND(SUM(total_neto), 2) as ingresos,\n",
    "        COUNT(DISTINCT cliente_id) as clientes_activos\n",
    "    FROM transacciones\n",
    "    WHERE estado = 'Completada'\n",
    "    GROUP BY DATE_FORMAT(fecha_transaccion, 'yyyy-MM')\n",
    "    ORDER BY anio_mes\n",
    "\"\"\")\n",
    "\n",
    "display(df_tendencia_mensual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3931f191-bc4c-44bf-8b16-4fa9412123bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 3: DistribuciÃ³n por CategorÃ­a (Pie Chart)\n",
    "# ============================================================\n",
    "# InstrucciÃ³n: Pie Chart con categoria como keys\n",
    "\n",
    "df_categoria = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.categoria,\n",
    "        COUNT(*) as numero_ventas,\n",
    "        ROUND(SUM(t.total_neto), 2) as ingresos,\n",
    "        ROUND(SUM(t.total_neto) * 100.0 / (SELECT SUM(total_neto) FROM transacciones WHERE estado = 'Completada'), 2) as porcentaje\n",
    "    FROM transacciones t\n",
    "    JOIN productos p ON t.producto_id = p.producto_id\n",
    "    WHERE t.estado = 'Completada'\n",
    "    GROUP BY p.categoria\n",
    "    ORDER BY ingresos DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3de793c-89c0-4067-b108-21266a5a4c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 4: Top 10 Productos\n",
    "# ============================================================\n",
    "\n",
    "df_top_productos = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.nombre_producto,\n",
    "        p.categoria,\n",
    "        COUNT(*) as unidades_vendidas,\n",
    "        ROUND(SUM(t.total_neto), 2) as ingresos\n",
    "    FROM transacciones t\n",
    "    JOIN productos p ON t.producto_id = p.producto_id\n",
    "    WHERE t.estado = 'Completada'\n",
    "    GROUP BY p.producto_id, p.nombre_producto, p.categoria\n",
    "    ORDER BY ingresos DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(df_top_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20210fac-5348-4633-ba5e-3b8b7b058c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 5: AnÃ¡lisis por Segmento de Cliente\n",
    "# ============================================================\n",
    "\n",
    "df_segmento_canal = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.segmento,\n",
    "        t.canal_venta,\n",
    "        COUNT(*) as numero_ventas,\n",
    "        ROUND(SUM(t.total_neto), 2) as ingresos,\n",
    "        ROUND(AVG(t.total_neto), 2) as ticket_promedio\n",
    "    FROM transacciones t\n",
    "    JOIN clientes c ON t.cliente_id = c.cliente_id\n",
    "    WHERE t.estado = 'Completada'\n",
    "    GROUP BY c.segmento, t.canal_venta\n",
    "    ORDER BY c.segmento, ingresos DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_segmento_canal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609a4598-2b86-4dfb-b5a1-58faa5450340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 6: Ventas por DÃ­a de la Semana\n",
    "# ============================================================\n",
    "\n",
    "df_dia_semana = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DAYOFWEEK(fecha_transaccion) as dia_num,\n",
    "        CASE DAYOFWEEK(fecha_transaccion)\n",
    "            WHEN 1 THEN 'Domingo'\n",
    "            WHEN 2 THEN 'Lunes'\n",
    "            WHEN 3 THEN 'Martes'\n",
    "            WHEN 4 THEN 'MiÃ©rcoles'\n",
    "            WHEN 5 THEN 'Jueves'\n",
    "            WHEN 6 THEN 'Viernes'\n",
    "            WHEN 7 THEN 'SÃ¡bado'\n",
    "        END as dia_semana,\n",
    "        COUNT(*) as ventas,\n",
    "        ROUND(SUM(total_neto), 2) as ingresos\n",
    "    FROM transacciones\n",
    "    WHERE estado = 'Completada'\n",
    "    GROUP BY DAYOFWEEK(fecha_transaccion)\n",
    "    ORDER BY dia_num\n",
    "\"\"\")\n",
    "\n",
    "display(df_dia_semana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1062de80-f264-4af8-a87e-bcc33250858a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 7: MÃ©todos de Pago\n",
    "# ============================================================\n",
    "\n",
    "df_metodo_pago = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        metodo_pago,\n",
    "        COUNT(*) as transacciones,\n",
    "        ROUND(SUM(total_neto), 2) as ingresos,\n",
    "        ROUND(AVG(total_neto), 2) as ticket_promedio\n",
    "    FROM transacciones\n",
    "    WHERE estado = 'Completada'\n",
    "    GROUP BY metodo_pago\n",
    "    ORDER BY transacciones DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_metodo_pago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7008c64d-81e1-4963-ad30-655235aed756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÃ“N 8: Ventas por Ciudad\n",
    "# ============================================================\n",
    "\n",
    "df_ciudades = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.ciudad,\n",
    "        COUNT(DISTINCT c.cliente_id) as clientes,\n",
    "        COUNT(t.transaccion_id) as transacciones,\n",
    "        ROUND(SUM(t.total_neto), 2) as ingresos,\n",
    "        ROUND(SUM(t.total_neto) / COUNT(DISTINCT c.cliente_id), 2) as valor_por_cliente\n",
    "    FROM clientes c\n",
    "    LEFT JOIN transacciones t ON c.cliente_id = t.cliente_id AND t.estado = 'Completada'\n",
    "    GROUP BY c.ciudad\n",
    "    ORDER BY ingresos DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_ciudades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08d6c18-4644-4b75-b928-709c2c8ed552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ¯ Parte 6: Proyecto Final - AnÃ¡lisis Avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199c63ef-9620-477f-986c-62102a7af1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÃLISIS RFM (Recency, Frequency, Monetary)\n",
    "# ============================================================\n",
    "print(\"ğŸ“Š ANÃLISIS RFM - SegmentaciÃ³n de Clientes\")\n",
    "\n",
    "df_rfm = spark.sql(\"\"\"\n",
    "    WITH cliente_metricas AS (\n",
    "        SELECT \n",
    "            t.cliente_id,\n",
    "            DATEDIFF(CURRENT_DATE(), MAX(t.fecha_transaccion)) as recency,\n",
    "            COUNT(DISTINCT t.transaccion_id) as frequency,\n",
    "            ROUND(SUM(t.total_neto), 2) as monetary\n",
    "        FROM transacciones t\n",
    "        WHERE t.estado = 'Completada'\n",
    "        GROUP BY t.cliente_id\n",
    "    ),\n",
    "    \n",
    "    rfm_scores AS (\n",
    "        SELECT \n",
    "            cliente_id,\n",
    "            recency,\n",
    "            frequency,\n",
    "            monetary,\n",
    "            NTILE(5) OVER (ORDER BY recency DESC) as r_score,\n",
    "            NTILE(5) OVER (ORDER BY frequency) as f_score,\n",
    "            NTILE(5) OVER (ORDER BY monetary) as m_score\n",
    "        FROM cliente_metricas\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        r.*,\n",
    "        CONCAT(r_score, f_score, m_score) as rfm_segment,\n",
    "        CASE \n",
    "            WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'Champions'\n",
    "            WHEN r_score >= 4 AND f_score >= 3 THEN 'Loyal Customers'\n",
    "            WHEN r_score >= 4 AND f_score <= 2 THEN 'New Customers'\n",
    "            WHEN r_score <= 2 AND f_score >= 3 AND m_score >= 3 THEN 'At Risk'\n",
    "            WHEN r_score <= 2 AND f_score <= 2 THEN 'Lost'\n",
    "            ELSE 'Regular'\n",
    "        END as customer_segment\n",
    "    FROM rfm_scores r\n",
    "\"\"\")\n",
    "\n",
    "# Resumen de segmentos\n",
    "df_rfm_resumen = (df_rfm\n",
    "    .groupBy(\"customer_segment\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_clientes\"),\n",
    "        spark_round(spark_avg(\"monetary\"), 2).alias(\"valor_promedio\"),\n",
    "        spark_round(spark_avg(\"frequency\"), 1).alias(\"frecuencia_promedio\")\n",
    "    )\n",
    "    .orderBy(col(\"num_clientes\").desc())\n",
    ")\n",
    "\n",
    "display(df_rfm_resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d7eb78-0a32-4b28-b004-a042ece6739c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÃLISIS DE COHORTS\n",
    "# ============================================================\n",
    "print(\"ğŸ“ˆ ANÃLISIS DE COHORTS - RetenciÃ³n por mes de primera compra\")\n",
    "\n",
    "df_cohorts = spark.sql(\"\"\"\n",
    "    WITH primera_compra AS (\n",
    "        SELECT \n",
    "            cliente_id,\n",
    "            DATE_FORMAT(MIN(fecha_transaccion), 'yyyy-MM') as cohort_mes\n",
    "        FROM transacciones\n",
    "        WHERE estado = 'Completada'\n",
    "        GROUP BY cliente_id\n",
    "    ),\n",
    "    \n",
    "    actividad_mensual AS (\n",
    "        SELECT \n",
    "            t.cliente_id,\n",
    "            pc.cohort_mes,\n",
    "            DATE_FORMAT(t.fecha_transaccion, 'yyyy-MM') as mes_actividad\n",
    "        FROM transacciones t\n",
    "        JOIN primera_compra pc ON t.cliente_id = pc.cliente_id\n",
    "        WHERE t.estado = 'Completada'\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        cohort_mes,\n",
    "        mes_actividad,\n",
    "        COUNT(DISTINCT cliente_id) as clientes_activos\n",
    "    FROM actividad_mensual\n",
    "    GROUP BY cohort_mes, mes_actividad\n",
    "    ORDER BY cohort_mes, mes_actividad\n",
    "\"\"\")\n",
    "\n",
    "display(df_cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ae41bfc-a5ac-473e-b884-87d9f496087c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REPORTE EJECUTIVO FINAL\n",
    "# ============================================================\n",
    "\n",
    "df_reporte = spark.sql(\"\"\"\n",
    "    SELECT 'ğŸ“Š RESUMEN EJECUTIVO - VENTAS MULTICANAL' as reporte\n",
    "    UNION ALL SELECT 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'\n",
    "    UNION ALL \n",
    "    SELECT CONCAT('Total Ingresos: S/ ', FORMAT_NUMBER(SUM(total_neto), 2))\n",
    "    FROM transacciones WHERE estado = 'Completada'\n",
    "    UNION ALL\n",
    "    SELECT CONCAT('Total Transacciones: ', FORMAT_NUMBER(COUNT(*), 0))\n",
    "    FROM transacciones WHERE estado = 'Completada'\n",
    "    UNION ALL\n",
    "    SELECT CONCAT('Clientes Ãšnicos: ', FORMAT_NUMBER(COUNT(DISTINCT cliente_id), 0))\n",
    "    FROM transacciones WHERE estado = 'Completada'\n",
    "    UNION ALL\n",
    "    SELECT CONCAT('Ticket Promedio: S/ ', FORMAT_NUMBER(AVG(total_neto), 2))\n",
    "    FROM transacciones WHERE estado = 'Completada'\n",
    "\"\"\")\n",
    "\n",
    "display(df_reporte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66afa9b-b563-457a-9759-35f9c958ab86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# âœ… Checklist de Temas Cubiertos\n",
    "\n",
    "| Tema | Estado | SecciÃ³n |\n",
    "|------|--------|--------|\n",
    "| Arquitectura Spark (Driver, Executors) | âœ… | Parte 1.1 |\n",
    "| Lazy Evaluation | âœ… | Parte 1.2 |\n",
    "| Lectura/Escritura CSV | âœ… | Parte 2 |\n",
    "| Lectura/Escritura JSON | âœ… | Parte 2 |\n",
    "| Lectura/Escritura Parquet | âœ… | Parte 2 |\n",
    "| INNER JOIN | âœ… | Parte 3.1 |\n",
    "| LEFT JOIN | âœ… | Parte 3.1 |\n",
    "| AnÃ¡lisis Multicanal | âœ… | Parte 3.1 |\n",
    "| Subqueries | âœ… | Parte 3.2 |\n",
    "| CTEs (WITH clause) | âœ… | Parte 3.2 |\n",
    "| ROW_NUMBER | âœ… | Parte 4.1 |\n",
    "| RANK / DENSE_RANK | âœ… | Parte 4.1 |\n",
    "| LAG | âœ… | Parte 4.2 |\n",
    "| LEAD | âœ… | Parte 4.2 |\n",
    "| Running Totals | âœ… | Parte 4.3 |\n",
    "| Moving Averages | âœ… | Parte 4.3 |\n",
    "| Visualizaciones Nativas | âœ… | Parte 5 |\n",
    "| Proyecto Integrado (RFM, Cohorts) | âœ… | Parte 6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be06d69a-2b34-47a6-96f8-3dc060cef4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ‰ PROYECTO COMPLETADO ğŸ‰                     â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  Has completado el proyecto de AnÃ¡lisis de Ventas Multicanal    â•‘\n",
    "â•‘  en Databricks Free Edition (Serverless).                       â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  CaracterÃ­sticas usadas:                                        â•‘\n",
    "â•‘  âœ“ Serverless Compute (automÃ¡tico)                              â•‘\n",
    "â•‘  âœ“ Unity Catalog Volumes para archivos                          â•‘\n",
    "â•‘  âœ“ Python y SQL                                                 â•‘\n",
    "â•‘  âœ“ Visualizaciones nativas                                      â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Proyecto_Ventas_Databricks_Free_Edition",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
